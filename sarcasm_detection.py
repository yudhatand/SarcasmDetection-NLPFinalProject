# -*- coding: utf-8 -*-
"""Sarcasm Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yb4UlFqpnsKw363MHQD_HgdtEXOtIIS_

#**Deteksi Sarkasme di Berita Utama**

Jurnalisme elektronik yang didukung oleh media sosial telah menjadi salah satu sumber utama konsumsi informasi akhir-akhir ini. Banyak rumah media menggunakan cara-cara kreatif untuk memanfaatkan peningkatan penayangan pada postingan. Salah satu caranya adalah menggunakan tajuk sarkastik sebagai umpan klik. Sebuah model yang mampu memprediksi apakah suatu berita utama bersifat sarkastik atau tidak dapat berguna bagi rumah media untuk menganalisis pendapatan triwulanan mereka berdasarkan strategi. Selain itu, dari sudut pandang pembaca, mesin telusur dapat memanfaatkan informasi sarkasme ini dan bergantung pada preferensi pembaca, merekomendasikan artikel serupa kepada mereka. Tujuannya adalah membangun model untuk mendeteksi apakah suatu kalimat sarkastik atau tidak, menggunakan Bidirectional LSTM.

Kumpulan data News Headlines untuk Deteksi Sarkasme Kumpulan data dikumpulkan dari dua situs web berita, theonion.com dan huffingtonpost.com . Studi sebelumnya dalam Deteksi Sarkasme sebagian besar menggunakan kumpulan data Twitter yang dikumpulkan menggunakan pengawasan berbasis tagar tetapi kumpulan data tersebut berisik dalam hal label dan bahasa. Selain itu, banyak tweet adalah balasan untuk tweet lain dan mendeteksi sarkasme di dalamnya membutuhkan ketersediaan tweet kontekstual. Kumpulan data baru ini memiliki keunggulan sebagai berikut dibandingkan kumpulan data Twitter yang ada: Karena tajuk berita ditulis oleh para profesional secara formal, tidak ada kesalahan ejaan dan penggunaan informal. Ini mengurangi ketersebaran dan juga meningkatkan kemungkinan menemukan embeddings yang telah dilatih sebelumnya. Adapun Dataset yang digunakan pada projek ini dapat diakses pada link berikut: https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection  

Selain itu, karena satu-satunya tujuan TheOnion adalah untuk menerbitkan berita sarkastik, kami mendapatkan label berkualitas tinggi dengan kebisingan yang jauh lebih sedikit dibandingkan dengan kumpulan data Twitter. Berbeda dengan tweet yang membalas tweet lainnya, headline berita yang didapatkan bersifat self-contained. Ini akan membantu kami dalam memisahkan elemen sarkastik yang sebenarnya.

Setiap record terdiri dari tiga atribut:

- is_sarcastic : 1 if the record is sarcastic otherwise 0

- headline : the headline of the news article

- article_link : link to the original news article. Useful in collecting supplementary data.

##**Import Library**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd 
import os
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten
from tensorflow.keras.callbacks import EarlyStopping
from bs4 import BeautifulSoup

"""##**Upload Dataset**"""

from google.colab import files
files.upload()

"""##**Read Dataset**"""

# Load the JSON file
df = pd.read_json("/content/Sarcasm_Headlines_Dataset.json", lines=True)

df.info()

"""##**Countplot of Target feature**"""

sns.countplot(x='is_sarcastic',data=df)

df.isna().sum()

del df['article_link'] # Menghapus kolom ini karena tidak ada gunanya

import re  
import matplotlib.pyplot as plt  
from wordcloud import WordCloud, STOPWORDS

"""##**Word Cloud of headline that is Not Sarcastic**"""

plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(df[df.is_sarcastic == 0].headline))
plt.imshow(wc , interpolation = 'bilinear')

"""##**Word Cloud of headline that is Sarcastic**"""

plt.figure(figsize = (20,20)) # Text that is Sarcastic
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(df[df.is_sarcastic == 1].headline))
plt.imshow(wc , interpolation = 'bilinear')
plt.figure(figsize = (20,20)) # Text that is Sarcastic
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(df[df.is_sarcastic == 1].headline))
plt.imshow(wc , interpolation = 'bilinear')

"""##**Visualization of characters in text**"""

fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))
text_len=df[df['is_sarcastic']==1]['headline'].str.len()
ax1.hist(text_len)
ax1.set_title('Sarcastic text')
text_len=df[df['is_sarcastic']==0]['headline'].str.len()
ax2.hist(text_len, color='green')
ax2.set_title('Not Sarcastic text')
fig.suptitle('Characters in texts')
plt.show()

"""##**Visualization of words in texts**"""

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
text_len=df[df['is_sarcastic']==1]['headline'].str.split().map(lambda x: len(x))
ax1.hist(text_len)
ax1.set_title('Sarcastic text')
text_len=df[df['is_sarcastic']==0]['headline'].str.split().map(lambda x: len(x))
ax2.hist(text_len,color='green')
ax2.set_title('Not Sarcastic text')
fig.suptitle('Words in texts')
plt.show()

"""##**Visualization of average word length in each text**"""

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
word=df[df['is_sarcastic']==1]['headline'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1)
ax1.set_title('Sarcastic text')
word=df[df['is_sarcastic']==0]['headline'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')
ax2.set_title('Not Sarcastic text')
fig.suptitle('Average word length in each text')

"""##**Splitting the dataset**"""

from sklearn.model_selection import train_test_split
X=df['headline'].values
y=df['is_sarcastic'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)
vocab_size=10000
embedding_dim=16
max_length=32
trunc_type='post'
padding_type='post'
oov_tok='<oov>'

"""##**Preprocessing the dataset**"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer= Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(X_train)

word_index=tokenizer.word_index

training_sequences=tokenizer.texts_to_sequences(X_train)
training_padded=pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

testing_sequences=tokenizer.texts_to_sequences(X_test)
testing_padded=pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

"""##**Initializing the model**"""

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))
model.add(Flatten())

model.add(Dense(units=32,activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(units=10,activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(units=1,activation='sigmoid'))

opt = tf.keras.optimizers.Adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
model.summary()

early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

"""##**Training the model with Early stopping**"""

model_1 = model.fit(x=training_padded, y=y_train, batch_size=256, epochs=100,
          validation_data=(testing_padded, y_test),
          verbose=1,
          callbacks=[early_stop])

"""##**Evaluating the model**"""

score = model.evaluate(testing_padded, y_test, batch_size=64, verbose=1)
print('Test accuracy:', score[1])

"""##Kesimpulan
Kami mendapatkan tes akurasi 86% yang cukup bagus dengan kerugian 0,32. Kami dapat lebih meningkatkan model kami menggunakan lapisan yang lebih kompleks. Mempertimbangkannya sebagai model dasar maka perlu dioptimalkan lebih lanjut untuk mendapatkan kinerja yang lebih baik.
"""